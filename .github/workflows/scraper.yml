name: Scrape and Insert Medicines

on:
  schedule:
    - cron: '0 */12 * * *'
  workflow_dispatch:

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
      - name: ðŸ”„ Checkout repository
        uses: actions/checkout@v3

      - name: ðŸ¦• Set up Deno 1.43.6
        uses: denoland/setup-deno@v1
        with:
          deno-version: "1.43.6"

      - name: ðŸ§ª Confirm Deno version
        run: deno --version

      - name: ðŸ Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: ðŸ“¦ Install Python dependencies
        run: |
          pip install beautifulsoup4 playwright httpx
          playwright install

      - name: ðŸ—‚ï¸ Create .env file
        run: |
          cat > .env << 'EOF'
          MONGODB_URI=${{ secrets.MONGODB_URI }}
          JWT_SECRET=${{ secrets.JWT_SECRET }}
          GMAIL_USER=${{ secrets.GMAIL_USER }}
          GMAIL_PASS=${{ secrets.GMAIL_PASS }}
          BASE_URL=${{ secrets.BASE_URL }}
          EOF
          echo "âœ… .env file created:"
          cat .env | sed 's/=.*/=***/'
        working-directory: MediSearch

      - name: ðŸ“ Debug directory structure
        run: |
          echo "Current directory: $(pwd)"
          ls -la
          echo "ðŸ” Checking important files:"
          test -f deno.json && echo "âœ… deno.json found"
          test -f deno.lock && echo "âœ… deno.lock found"
          test -d scraping_tasks && echo "âœ… scraping_tasks directory found"
        working-directory: MediSearch

      - name: ðŸ”— Extract URLs (if extractor exists)
        run: |
          if [ -f "url_extractor/extract_urls.py" ]; then
            echo "ðŸ” Running URL extractor..."
            python3 url_extractor/extract_urls.py
          else
            echo "ðŸ“ Creating empty URL files for scrapers..."
            mkdir -p url_extractor/extracted_urls
            echo '{}' > url_extractor/extracted_urls/ahumada_urls.json
            echo '{}' > url_extractor/extracted_urls/cruzverde_urls.json
            echo '{}' > url_extractor/extracted_urls/salcobrand_urls.json
          fi
        working-directory: MediSearch

      - name: ðŸš€ Run scrape-and-insert
        run: deno task scrape-and-insert
        working-directory: MediSearch

      - name: ðŸ§¹ Clean temporary folders after insert
        run: |
          echo "ðŸ§¹ Removing old product updates..."
          rm -rf Scrapers_MediSearch/product_updates
        working-directory: MediSearch

      - name: ðŸ“¤ Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-logs
          path: MediSearch/logs/
